# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hMcqez5PuBZM2WIAXt_qhRjI03P8HoAs
"""

!pip install openai pandas gradio

!pip install transformers pandas gradio

import openai

# Set your OpenAI API key
openai.api_key = 'your_openai_api_key'

def query_gpt3(prompt):
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=150,
        n=1,
        stop=None,
        temperature=0.5,
    )
    return response.choices[0].text.strip()

from transformers import pipeline

# Load the model
model = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')

def query_open_source_llm(prompt):
    response = model(prompt, max_length=150, num_return_sequences=1)
    return response[0]['generated_text'].strip()

import pandas as pd

def load_excel(file):
    df = pd.read_excel(file)
    return df

def get_insight_from_df(df, query):
    # For simplicity, we'll concatenate column names and first few rows to create context
    context = f"Data columns: {', '.join(df.columns)}\n\n"
    context += df.head().to_string()
    return context + "\n\n" + query

import gradio as gr

def chatbot_interface(file, user_query):
    # Load the dataset
    df = load_excel(file)

    # Generate the prompt based on the user's query and the data context
    context_query = get_insight_from_df(df, user_query)

    # Get the response from the LLM
    response = query_gpt3(context_query)  # or query_open_source_llm for open-source models

    return response

# Create the Gradio interface
interface = gr.Interface(
    fn=chatbot_interface,
    inputs=[gr.File(label="Upload Excel File"), gr.Textbox(label="Your Question")],
    outputs="text",
    title="Excel Data Analytical Insights Chatbot"
)

# Launch the interface
interface.launch()

